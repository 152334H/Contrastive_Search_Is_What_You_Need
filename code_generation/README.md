## Code Generation

****

### Catalogue:
* <a href='#evaluation_setup'>1. Evaluation Setup</a>
* <a href='#inference'>2. Inference with Different Decoding Methods</a>

****
<span id='evaluation_setup'/>

#### 1. Evaluation Setup:

First, we should install the evaluation environment `human-eval` for HumanEval benchmark following the official instructions [[here]](https://github.com/openai/human-eval). After installation, please run a quick sanity check as suggested in the [official instructions](https://github.com/openai/human-eval#usage).

****
<span id='inference'/>

#### 2. Inference with Different Decoding Methods:
